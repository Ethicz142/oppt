# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
saveParticles = true

[plugins]
heuristicPlugin = libDefaultHeuristicPlugin.so

planningRewardPlugin = libtigerRewardPlugin.so
executionRewardPlugin = libtigerRewardPlugin.so

planningTerminalPlugin = libtigerTerminalPlugin.so
executionTerminalPlugin = libtigerTerminalPlugin.so

planningTransitionPlugin = libtigerTransitionPlugin.so
executionTransitionPlugin = libtigerTransitionPlugin.so

planningObservationPlugin = libtigerObservationPlugin.so
executionObservationPlugin = libtigerObservationPlugin.so

executionInitialBeliefPlugin = libtigerInitialBeliefPlugin.so
planningInitialBeliefPlugin = libtigerInitialBeliefPlugin.so


[observationPluginOptions]
observationError = 0.15

[rewardPluginOptions]
correctGuessReward = 10
wrongGuessPenalty = -100
stepPenalty = -1

[problem]
# Number of simulation runs
nRuns = 3

# Maximum number of steps to reach the goal
nSteps = 150

# # The planning environment SDF
# planningEnvironmentPath = TigerEnvironment.sdf

# # The execution environment SDF
# executionEnvironmentPath = TigerEnvironment.sdf

# # The robot SDF model
robotName = Tiger

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.99

# Using state- action- and observation spaces that are normalized to [0, 1]
# normalizedSpaces = true
normalizedSpaces = false

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

[state]
#3 states, tiger is really behind the left door or the right
additionalDimensions = 1
#1: tiger is left, #2: tiger is right
additionalDimensionLimits = [[1, 2]]

[action]
#3 actions, left (1), right (2), listen (3)
additionalDimensions = 1
#encodes 3 discrete actions
additionalDimensionLimits = [[1, 3]]

[observation]
#observation is tiger is left (1) or tiger is right (2), null observation is (0)
additionalDimensions = 1
additionalDimensionLimits = [[0, 2]]

[changes]
hasChanges = false
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 3000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchStrategy = ucb(2.0)

estimator = max()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 3

observationType = discrete
numInputStepsObservation = 3

# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 1.2

[simulation]
interactive = false 
particlePlotLimit = 0
