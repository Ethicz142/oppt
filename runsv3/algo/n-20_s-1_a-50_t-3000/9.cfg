# General-purpose settings.
verbose = true
logPath = log/algo/n-20_s-1_a-50_t-3000/9
overwriteExistingLogFiles = true
saveParticles = true

[plugins]
heuristicPlugin = libDefaultHeuristicPlugin.so

planningRewardPlugin = libcuttingV2RewardPlugin.so
executionRewardPlugin = libcuttingV2RewardPlugin.so

planningTerminalPlugin = libcuttingV2TerminalPlugin.so
executionTerminalPlugin = libcuttingV2TerminalPlugin.so

planningTransitionPlugin = libcuttingV2TransitionPlugin.so
executionTransitionPlugin = libcuttingV2TransitionPlugin.so

planningObservationPlugin = libcuttingV2ObservationPlugin.so
executionObservationPlugin = libcuttingV2ObservationPlugin.so

executionInitialBeliefPlugin = libcuttingV2InitialBeliefPlugin.so
planningInitialBeliefPlugin = libcuttingV2InitialBeliefPlugin.so


[observationPluginOptions]
damageErrorBound = 0.2
hardnessErrorBound = 0.2
sharpnessErrorBound = 0.2

[rewardPluginOptions]
objectCutReward = 100
damagedPenalty = -100
uncutPenalty = -10
scanPenalty = -2
cutPenalty = -1

[initialBeliefPluginOptions]
numberOfSuitableCutters = 1

[generalOptions]
numberOfCutters = 20
trueObjectHardnessRange = [0.432, 0.583]
trueObjectSharpnessRange = [0.355, 0.519]

[problem]
# Number of simulation runs
nRuns = 50

# Maximum number of steps to reach the goal
nSteps = 150

# # The robot SDF model
robotName = CuttingV2

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.95

# The variable which controls weight of the qValue vs the new one
a = 0.5

# Using state- action- and observation spaces that are normalized to [0, 1]
# normalizedSpaces = true
normalizedSpaces = false

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 3000

# --- State ---
# 1st dimension:
#   0: Uncut
#   1: Cut 
#   2: Damaged

# when n != 0

# 2n dimension:
#   0 - 1: Hardness of cutter n 
# 2n + 1 dimension:
#   0 - 1: Sharpness of cutter n
[state]
additionalDimensions = 41
additionalDimensionLimits = [[0, 2], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]

# --- Action ---
# -1: scan for hardness
# 0: scan for sharpness
# n: cut with cutter n 

[action]
additionalDimensions = 1
additionalDimensionLimits = [[-1, 20]]

# --- Observation ---
# 1st dimension (any cut action):
#   0 - 9: Discrete observations, see POMDP formulation

# when n != 0
# For scan:
# 2n dimension:
#   0 - 1: Hardness of cutter n 
# 2n + 1 dimension:
#   0 - 1: Sharpness of cutter n

[observation]
additionalDimensions = 41
additionalDimensionLimits = [[0, 9], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]

[changes]
hasChanges = false
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 3000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchStrategy = ucb(2.0)

estimator = max()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 22

observationType = continuous

# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 0.01

[simulation]
interactive = false 
particlePlotLimit = 0